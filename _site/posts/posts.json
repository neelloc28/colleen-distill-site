[
  {
    "path": "posts/2021-04-01-tidytext-tutorial/",
    "title": "Tidytext Tutorial",
    "description": "Here is a tutorial for some of the key functions in the tidytext R package.  We show examples of how these functions aid with text analysis on a small dataset, and we then provide some exercises using data of Russian Troll Tweets for you to try on your own.",
    "author": [
      {
        "name": "Colleen Minnihan, Jackson Tak, and Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-04-02",
    "categories": [],
    "contents": "\n1. Tutorial\nIntroduction\nToday, we will talk about the TidyText package. The tidytext packages allows us to effectively do text mining tasks by converting text data into tidy formats. We can then incorporated these tidy data sets with other tools in data science or machine learning.\nIn this tutorial, we will explain some of the key functions used in the tidytext package by using a simple example. We will then ask you to work on questions using a different data set: Russian Troll Tweets.\n\nGetting Started\nLoading Libraries\nFirst, as always, we have to load libraries and install packages. You will encounter some familiar packages, but the main libraries required to do text mining with tidytext are listed below:\n\n\n# load libraries \nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(textdata)\nlibrary(reshape2)\nlibrary(wordcloud) # for wordcloud\nlibrary(stopwords)\n\n\n\n\nLooking at the Data Set\nAfter loading all the relevant libraries, we will look at a simple data set that we created. The text data set includes 9 different sentences or strings.\n\n\ntext<-c(\"Lisa is awesome\",\"I love Data Science\",\"I hate cilantro\", \"I dislike vegetables\", \"I enjoy smiling\", \"I hate exams\", \"I love travelling\", \"The weather is so nice!\", \"Can I have an apple?\")\n\ntext\n\n\n[1] \"Lisa is awesome\"         \"I love Data Science\"    \n[3] \"I hate cilantro\"         \"I dislike vegetables\"   \n[5] \"I enjoy smiling\"         \"I hate exams\"           \n[7] \"I love travelling\"       \"The weather is so nice!\"\n[9] \"Can I have an apple?\"   \n\nThe first task before doing text analysis is converting our data set into a tibble data frame by using the tibble function. A tibble is just another class of data frames in R, which allows us to work with tidy functions, as it does not convert strings to factors or use row names.\nTake a look at how we were able to achieve this below:\n\n\n# Converting to a tibble data frame \ntext_df<- tibble(line = 1:length(text), text = text)\n\n# Tibble data frame\ntext_df\n\n\n# A tibble: 9 x 2\n   line text                   \n  <int> <chr>                  \n1     1 Lisa is awesome        \n2     2 I love Data Science    \n3     3 I hate cilantro        \n4     4 I dislike vegetables   \n5     5 I enjoy smiling        \n6     6 I hate exams           \n7     7 I love travelling      \n8     8 The weather is so nice!\n9     9 Can I have an apple?   \n\nAfter we successfully convert our data set into a tibble data frame, we want to extract individual words and put them into a data frame so that each row has one token.\nEach token can be interpreted as a word or unit of text. By utilizing tidytext’s unnest_tokens function, we can break the text into individual tokens. This process is also known as the tokenization process. Note that this function eliminates all the punctuation marks.\n\n\n# Tokenization process\ntext_unnest <- text_df %>% \n  unnest_tokens(word, text)\n\n# Tokenized data \ntext_unnest\n\n\n# A tibble: 32 x 2\n    line word    \n   <int> <chr>   \n 1     1 lisa    \n 2     1 is      \n 3     1 awesome \n 4     2 i       \n 5     2 love    \n 6     2 data    \n 7     2 science \n 8     3 i       \n 9     3 hate    \n10     3 cilantro\n# … with 22 more rows\n\nAs we see above, the unnest_tokens function takes in two parameters. The first parameter, the output column, will be the name of the output column or individual words. In our exercise, we will call it word.\nThe second parameter will take the tokenized column name from the tibble data frame.\nBefore starting our text analysis, we will also have to exclude stop words such as “is”, “I”, “the”, “about”, “an”, “the”, etc. The tidytext package provides a tibble data set called stop_words that includes different stop words.\nYou can take a peek at some of the stop words in the stop_words data set:\n\n\n# Look at the stop words data set\nhead(stop_words)\n\n\n# A tibble: 6 x 2\n  word      lexicon\n  <chr>     <chr>  \n1 a         SMART  \n2 a's       SMART  \n3 able      SMART  \n4 about     SMART  \n5 above     SMART  \n6 according SMART  \n\nFinally, we will use a familiar function, anti_join, to clean the data set. This is the last step of the data cleaning process that we need to do before moving forward with the analysis. By joining the two data sets, stop_words and text_unnest, we can arrive at a clean data set.\n\n\n# Join the two data sets together to only extract meaningful words\ntext_clean <- text_unnest %>% \n anti_join(stop_words) \n\n# Cleaned data set \ntext_clean\n\n\n# A tibble: 18 x 2\n    line word      \n   <int> <chr>     \n 1     1 lisa      \n 2     1 awesome   \n 3     2 love      \n 4     2 data      \n 5     2 science   \n 6     3 hate      \n 7     3 cilantro  \n 8     4 dislike   \n 9     4 vegetables\n10     5 enjoy     \n11     5 smiling   \n12     6 hate      \n13     6 exams     \n14     7 love      \n15     7 travelling\n16     8 weather   \n17     8 nice      \n18     9 apple     \n\n\nData Analysis\nWith the cleaned version of the data, we will begin our analysis. In this tutorial, we mainly focus on the sentiment analysis and creating relevant visualizations using word cloud\n\nSentiment Analysis\nBefore beginning the sentiment analysis, let’s take a step back and understand the purpose of sentiment analysis. Sentiment analysis “provides a way to understand the attitudes and opinions expressed in texts”.\nTo conduct sentiment analysis, we will be first looking at the get_sentiments(). By using this function within the tidytext package, we can look at words with negative and positive sentiments. Note that this function takes in a lexicon parameter. There are three sentiment lexicons that we can use:\nbing (default): positive vs. negative\nnrc: assigns yes vs. no to positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust cateogries.\nAFINN: assigns scores from -5 to 5 depending on the sentiment of the word.\nFor this tutorial, we will use bing for simplicity. Feel free to play with other lexicons.\n\n\n# Sentiments \nsentiments <- get_sentiments(\"bing\")\n\n# Negative words\nsentiments %>% \n  filter(sentiment == \"negative\") %>% \n  head()\n\n\n# A tibble: 6 x 2\n  word       sentiment\n  <chr>      <chr>    \n1 2-faces    negative \n2 abnormal   negative \n3 abolish    negative \n4 abominable negative \n5 abominably negative \n6 abominate  negative \n\n# Positive words\nsentiments %>% \n  filter(sentiment == \"positive\") %>% \n  head()\n\n\n# A tibble: 6 x 2\n  word       sentiment\n  <chr>      <chr>    \n1 abound     positive \n2 abounds    positive \n3 abundance  positive \n4 abundant   positive \n5 accessable positive \n6 accessible positive \n\nNext, to figure out the sentiment for each word, we will join the two data sets: sentiments and text_clean. Note that some words in the text_clean data set are dropped i.e., “exams”, “weather”, “apple”, etc.\n\n\nsentiment_data <- text_clean %>%\n  inner_join(sentiments) \n\nsentiment_data\n\n\n# A tibble: 9 x 3\n   line word    sentiment\n  <int> <chr>   <chr>    \n1     1 awesome positive \n2     2 love    positive \n3     3 hate    negative \n4     4 dislike negative \n5     5 enjoy   positive \n6     5 smiling positive \n7     6 hate    negative \n8     7 love    positive \n9     8 nice    positive \n\nUsing this sentiment data, we will now begin the sentiment analysis by using some functions that we already know.\n\n\nsentiment_data %>%  \n  count(sentiment)\n\n\n# A tibble: 2 x 2\n  sentiment     n\n  <chr>     <int>\n1 negative      3\n2 positive      6\n\nFrom above, We see that there are 3 negative words and 6 positive words.\n\nMaking cool visualizations using WordCloud\nWith the above analysis in mind, we can also create interesting visualizations using the wordcloud package.\nUsing the text_clean data (without sentiments), we can create this nice visualization which shows us all the words in the data set. Note that the size of “hate” and “love” are bigger than other words. This is because they appear more than other words.\n\n\ntext_clean %>%\n  count(word) %>%\n  with(wordcloud(word, n))\n\n\n\n\nUsing the data set that includes sentiments, we can also create a similar visualization.\n\n\nsentiment_data %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(\"red\", \"green\"))\n\n\n\n\nCompared to the first wordcloud visualization, this one allows us to make a comparison. Through this visualization, we can understand what words have positive and negative associations and what words appear more than the others within their specific groups.\n\nOther notable functions in the tidytext package\nFor the purpose of this tutorial, we selected some of the tidytext functions. However, there are many more functions that you might be interested in using for your final project if your group were to do some text mining and analysis.\nOther functions and their documentations can be found in the link below:\nhttps://cran.r-project.org/web/packages/tidytext/tidytext.pdf\n\n2. Exercises\nNow you will try using tidytext on a new dataset about Russian Troll tweets.\n\nRead about the data\nThese are tweets from Twitter handles that are connected to the Internet Research Agency (IRA), a Russian “troll factory.” The majority of these tweets were posted from 2015-2017, but the datasets encompass tweets from February 2012 to May 2018.\nThree of the main categories of troll tweet that we will be focusing on are Left Trolls, Right Trolls, and News Feed. Left Trolls usually pretend to be BLM activists, aiming to divide the democratic party (in this context, being pro-Bernie so that votes are taken away from Hillary). Right trolls imitate Trump supporters, and News Feed handles are “local news aggregators,” typically linking to legitimate news.\nFor our upcoming analyses, some important variables are:\nauthor (handle sending the tweet)\ncontent (text of the tweet)\nlanguage (language of the tweet)\npublish_date (date and time the tweet was sent)\nVariable documentation can be found on Github and a more detailed description of the dataset can be found in this fivethirtyeight article.\nBecause there are 12 datasets containing 2,973,371 tweets sent by 2,848 Twitter handles in total, we will be using three of these datasets (one from a Right troll, one from a Left troll, and one from a News Feed account).\n\n1. Read in Troll Tweets Dataset\n\n\n# Download the file from github and place in the same project folder\n\ntroll_tweets<- read.csv(\"IRAhandle_tweets_12.csv\")\n\n\n\n\n2. Basic Data Cleaning and Exploration\nRemove rows where the tweet was in a language other than English\nReport the dimensions of the dataset\nCreate two or three basic exploratory plots of the data (ex. plot of the different locations from which tweets were posted, plot of the account category of a tweet)\n\n\n\n\n3. Unnest Tokens\nWe want each row to represent a word from a tweet, rather than an entire tweet.\n\ntroll_tweets_untoken <- troll_tweets1 %>%\n  unnest_tokens(???,???)\n\ntroll_tweets_untoken\n\n\n4. Remove stopwords\n\n\n#get rid of stopwords (the, and, etc.)\ntroll_tweets_cleaned <- troll_tweets_untoken %>%\n  anti_join(stop_words)\n\n\n\nTake a look at the troll_tweets_cleaned dataset. Are there any other words/letters/numbers that we want to eliminate that weren’t taken care of by stop_words?\n\n#get rid of http, https, t.co, rt, amp, single number digits, and singular letters\ntroll_tweets_cleaned <- troll_tweets_cleaned %>%\n  filter(word != ????)\n\n\n5. Look at a subset of the tweets to see how often the top words appear.\n\ntroll_tweets_small <- troll_tweets_cleaned %>%\n  count(??) %>%\n  slice_max(order_by = n, n = 50) # 50 most occurring words\n\n# visualize the number of times the 50 top words appear\nggplot(troll_tweets_small, aes(y = fct_reorder(word,n), x = n)) +\n  geom_col()\n\n\n6. Sentiment Analysis\nGet the sentiments using the “bing” parameter (which classifies words into “positive” or “negative”)\nReport how many positive and negative words there are in the dataset. Are there more positive or negative words, and why do you think this might be?\n\n\n\n# look at sentiment\nget_sentiments(\"bing\")\n\n# assign a sentiment to each word that has one associated\ntroll_tweets_sentiment <- troll_tweets_cleaned %>%\n  inner_join(???)\n\n# count the sentiments\ntroll_tweets_sentiment %>% \n  ???\n\n7. Using the troll_tweets_small dataset, make a wordcloud:\nThat is sized by the number of times that a word appears in the tweets\nThat is colored by sentiment (positive or negative)\n\n# make a wordcloud where the size of the word is based on the number of times the word appears across the tweets\n\ntroll_tweets_small %>%\n  with(wordcloud(word, n, max.words = ??))\n\n# make a wordcloud colored by sentiment\n\ntroll_tweets_sentiment %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(colors = c(??,??),\n                   max.words = ??)\n\nAre there any words whose categorization as “positive” or “negative” surprised you?\n\nSources\nhttps://cran.r-project.org/web/packages/tidytext/tidytext.pdf\nhttps://www.tidytextmining.com/\nhttps://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets\nhttps://www.youtube.com/watch?v=-uVo0Xvmimw\nhttps://github.com/fivethirtyeight/russian-troll-tweets/\n\n\n\n",
    "preview": "posts/2021-04-01-tidytext-tutorial/tidytext-tutorial_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-04-05T12:37:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-21-post-4/",
    "title": "What Affects Independence Among the Elderly?",
    "description": "We used Survival Analysis to see what factors (such as race, income, and overall health) impact the onset of dependence doing various tasks among a sample of elderly people across the United States.",
    "author": [
      {
        "name": "Colleen Minnihan and Analeidi Barrera",
        "url": {}
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\nHere is the blog post where we discuss our process and interesting findings.\n(Preview image source)\n\n\n\n",
    "preview": "posts/2021-03-21-post-4/elderly.jpg",
    "last_modified": "2021-03-22T10:33:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-post-3/",
    "title": "Does Straighter = Safer?",
    "description": "Here I use Causal Inference to investigate if sexual orientation impacts college students' perceived safety in the Twin Cities at nighttime.",
    "author": [
      {
        "name": "Colleen Minnihan",
        "url": {}
      }
    ],
    "date": "2020-11-22",
    "categories": [],
    "contents": "\nTake a look at this website where I talk about my process and findings.\n(Preview image source)\n\n\n\n",
    "preview": "posts/2021-03-20-post-3/pride-flag.jpeg",
    "last_modified": "2021-03-22T10:33:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-post-2/",
    "title": "Employee Attrition: Why do people leave their jobs?",
    "description": "Here, we wanted to see which potential (or current) employees have higher likelihoods of leaving a job without being replaced, and which are more likely to stay. To investigate this question, we used Machine Learning concepts to pick the ideal model and parameters for our dataset.",
    "author": [
      {
        "name": "Colleen, Ikran, Sebastian, and Amritha",
        "url": {}
      }
    ],
    "date": "2020-04-21",
    "categories": [],
    "contents": "\n\n\n\n\nIntroduction\n\nMany people remain at their jobs for a long time, but some people inevitably end up leaving. Wouldn’t it be beneficial to be able to tell which potential (or current) employees have a higher likelihood of leaving the job without being replaced, and which are likely to stay?\nThis knowledge could be used by the employers for good. For example, they could see what factors influence employee attrition that can be changed to better the employee’s experience, such as whether or not the employee works overtime. It could also be used for not so good reasons, if a model predicts that a potential employee is likely to quit, and that leads to them not even being considered for the job.\nWe wanted to explore this more, to see if we could accurately predict whether or not an employee will leave their job. We used Kaggle’s (fictional) attrition dataset, which contains data from 1470 employees. In the original dataset, about 83.9% of employees were replaced, while 16.1% resulted in attrition. These percentages are shown in the plot below.\n\n\nggplot(data = attrition, aes(x=Attrition)) +\n  geom_bar(color = 'blue', fill = 'lightblue') +\n  ggtitle(\"Employee Attrition in Full Dataset\")\n\n\n\n\nThe dataset included 35 variables:\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"EnvironmentSatisfaction\"  \"Gender\"                  \n[13] \"HourlyRate\"               \"JobInvolvement\"          \n[15] \"JobLevel\"                 \"JobRole\"                 \n[17] \"JobSatisfaction\"          \"MaritalStatus\"           \n[19] \"MonthlyIncome\"            \"MonthlyRate\"             \n[21] \"NumCompaniesWorked\"       \"Over18\"                  \n[23] \"OverTime\"                 \"PercentSalaryHike\"       \n[25] \"PerformanceRating\"        \"RelationshipSatisfaction\"\n[27] \"StandardHours\"            \"StockOptionLevel\"        \n[29] \"TotalWorkingYears\"        \"TrainingTimesLastYear\"   \n[31] \"WorkLifeBalance\"          \"YearsAtCompany\"          \n[33] \"YearsInCurrentRole\"       \"YearsSinceLastPromotion\" \n[35] \"YearsWithCurrManager\"    \n\n\nData Cleaning\n\nTo clean the data, we recoded the levels of the WorkLifeBalance and EnvironmentSatisfaction variables to be more meaningful to the viewer, rather than just easily-misinterperable numbers. Originally these variables were coded as numbers 1-4, but we refactored them to take on their original values, e.g. “High” or “Good”.\n\nOur hypothesis\n\nWe then tried to brainstorm what variables (out of the 35 in the attrition data) might be important predictors of whether or not an employee will quit their job. We split the data into training and testing, and created exploratory plots of some of the variables using the training dataset. A few variables that we thought would be important were OverTime, YearsAtCompany, Age, WorkLifeBalance, and EnvironmentSatisfaction. The relationships of these variables to Attrition are visualized below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first model we built was a logistic regression model using these variables (OverTime, YearsAtCompany, Age, WorkLifeBalance, and EnvironmentSatisfaction) that we initially suspected to have an influence on attrition rates.\n1st model\n\n\nset.seed(253)\n\nattrition_mod1 <- train(\n    Attrition ~ OverTime + YearsAtCompany + Age + WorkLifeBalance + EnvironmentSatisfaction ,\n    data = attrition_train,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 5),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\n\nCall:\nNULL\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4023  -0.5910  -0.4175  -0.2728   3.3062  \n\nCoefficients:\n                                   Estimate Std. Error z value\n(Intercept)                         1.50312    0.52390   2.869\nOverTimeYes                         1.46045    0.18813   7.763\nYearsAtCompany                     -0.05950    0.01999  -2.976\nAge                                -0.04962    0.01094  -4.536\nWorkLifeBalanceGood                -0.58249    0.38613  -1.509\nWorkLifeBalanceBetter              -0.90099    0.36450  -2.472\nWorkLifeBalanceBest                -0.60660    0.44014  -1.378\nEnvironmentSatisfactionMedium      -1.10949    0.27721  -4.002\nEnvironmentSatisfactionHigh        -0.99689    0.24513  -4.067\n`EnvironmentSatisfactionVery High` -1.12372    0.24923  -4.509\n                                   Pr(>|z|)    \n(Intercept)                         0.00412 ** \nOverTimeYes                        8.30e-15 ***\nYearsAtCompany                      0.00292 ** \nAge                                5.72e-06 ***\nWorkLifeBalanceGood                 0.13141    \nWorkLifeBalanceBetter               0.01344 *  \nWorkLifeBalanceBest                 0.16814    \nEnvironmentSatisfactionMedium      6.27e-05 ***\nEnvironmentSatisfactionHigh        4.77e-05 ***\n`EnvironmentSatisfactionVery High` 6.52e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 919.16  on 1028  degrees of freedom\nResidual deviance: 787.87  on 1019  degrees of freedom\nAIC: 807.87\n\nNumber of Fisher Scoring iterations: 5\n\nWe can interpret the exponentiated coefficients of this model as multipliers to the odds of an employee leaving resulting in attrition. For example, the exponentiated coefficient on the OverTimeYes variable of 4.308 means that employees who worked overtime have their odds of leaving resulting in attrition multiplied by 4.308 compared to employees who did not work overtime.\n\nModel Evaluation\n\nTo evaluate this model, we looked at the accuracy rate, as well as the sensitivity and specificty. This model had an accuracy rate of 84.9%. This appears good at first glance, but it is very close to the no information rate (the rate at which a correct guess can be made with no information) of the data, which is 83.6%. Additionally, the sensitivity is 14.2% and the specificity is 98.7%. This means that the model is mostly guessing no attrition for every case and getting the actual no attrition cases right almost all the time and the actual attrition cases wrong almost all the time. We want our model to be able to detect attrition, so this isn’t the best model for us.\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  849 145\n       Yes  11  24\n                                         \n               Accuracy : 0.8484         \n                 95% CI : (0.825, 0.8698)\n    No Information Rate : 0.8358         \n    P-Value [Acc > NIR] : 0.1462         \n                                         \n                  Kappa : 0.1896         \n                                         \n Mcnemar's Test P-Value : <2e-16         \n                                         \n            Sensitivity : 0.14201        \n            Specificity : 0.98721        \n         Pos Pred Value : 0.68571        \n         Neg Pred Value : 0.85412        \n             Prevalence : 0.16424        \n         Detection Rate : 0.02332        \n   Detection Prevalence : 0.03401        \n      Balanced Accuracy : 0.56461        \n                                         \n       'Positive' Class : Yes            \n                                         \n\n\n\n# Accuracy rate of 84.9%... BUT it isn't much better than the No Information Rate (predicting these by chance) of 83.6%...\n# we care more about the sensitivity, which is the percentage of correct predictions that people would leave their job out of all of the people who did end up leaving their job\n# sensitivity is .13; therefore, this model does not seem very good\n\n# CV accuracy of 84.5%\nattrition_mod1$results$Accuracy\n\n\n\nOur second model uses Logistic Regression using all variables except Over18 and StandardHours. These two variables only have one factor, so they are meaningless in our model-building.\n2nd Model\n\n\nset.seed(253)\n\n# Perform logistic regression\nattrition_allvars <- train(\n    Attrition ~ . ,\n    data = attrition_train %>% select(-Over18, -StandardHours),\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 5),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\nsummary(attrition_allvars)\n\n\n\nCall:\nNULL\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6753  -0.4695  -0.2278  -0.0714   3.4297  \n\nCoefficients: (1 not defined because of singularities)\n                                     Estimate Std. Error z value\n(Intercept)                        -1.154e+01  7.312e+02  -0.016\nAge                                -2.164e-02  1.664e-02  -1.300\nBusinessTravelTravel_Frequently     2.180e+00  5.308e-01   4.106\nBusinessTravelTravel_Rarely         1.275e+00  4.993e-01   2.553\nDailyRate                          -7.058e-04  2.801e-04  -2.520\n`DepartmentResearch & Development`  1.441e+01  7.312e+02   0.020\nDepartmentSales                     1.234e+01  7.312e+02   0.017\nDistanceFromHome                    4.932e-02  1.349e-02   3.657\nEducation                          -7.538e-02  1.093e-01  -0.690\n`EducationFieldLife Sciences`      -1.643e+00  1.031e+00  -1.593\nEducationFieldMarketing            -1.399e+00  1.087e+00  -1.287\nEducationFieldMedical              -1.505e+00  1.039e+00  -1.448\nEducationFieldOther                -1.577e+00  1.115e+00  -1.415\n`EducationFieldTechnical Degree`   -4.602e-01  1.041e+00  -0.442\nEmployeeCount                              NA         NA      NA\nEmployeeNumber                     -2.481e-04  1.914e-04  -1.296\nEnvironmentSatisfactionMedium      -1.367e+00  3.427e-01  -3.989\nEnvironmentSatisfactionHigh        -1.241e+00  3.110e-01  -3.991\n`EnvironmentSatisfactionVery High` -1.480e+00  3.140e-01  -4.714\nGenderMale                          5.581e-01  2.308e-01   2.418\nHourlyRate                          5.191e-03  5.615e-03   0.924\nJobInvolvement                     -5.855e-01  1.518e-01  -3.856\nJobLevel                            2.594e-02  3.989e-01   0.065\n`JobRoleHuman Resources`            1.562e+01  7.312e+02   0.021\n`JobRoleLaboratory Technician`      1.256e+00  5.556e-01   2.260\nJobRoleManager                      7.050e-01  9.822e-01   0.718\n`JobRoleManufacturing Director`    -1.729e-01  6.392e-01  -0.270\n`JobRoleResearch Director`         -2.320e+00  1.335e+00  -1.739\n`JobRoleResearch Scientist`         2.811e-01  5.734e-01   0.490\n`JobRoleSales Executive`            3.118e+00  1.399e+00   2.229\n`JobRoleSales Representative`       4.156e+00  1.463e+00   2.842\nJobSatisfaction                    -3.711e-01  1.017e-01  -3.648\nMaritalStatusMarried                3.528e-01  3.316e-01   1.064\nMaritalStatusSingle                 1.333e+00  4.317e-01   3.088\nMonthlyIncome                       7.089e-06  1.033e-04   0.069\nMonthlyRate                        -5.943e-08  1.564e-05  -0.004\nNumCompaniesWorked                  1.562e-01  4.980e-02   3.136\nOverTimeYes                         2.072e+00  2.454e-01   8.446\nPercentSalaryHike                   1.625e-02  4.975e-02   0.327\nPerformanceRating                  -3.353e-02  5.069e-01  -0.066\nRelationshipSatisfaction           -3.011e-01  1.037e-01  -2.903\nStockOptionLevel                   -2.418e-01  2.019e-01  -1.198\nTotalWorkingYears                  -4.195e-02  3.658e-02  -1.147\nTrainingTimesLastYear              -2.087e-01  9.326e-02  -2.237\nWorkLifeBalanceGood                -5.769e-01  4.806e-01  -1.200\nWorkLifeBalanceBetter              -1.289e+00  4.588e-01  -2.810\nWorkLifeBalanceBest                -8.765e-01  5.489e-01  -1.597\nYearsAtCompany                      9.716e-02  4.504e-02   2.157\nYearsInCurrentRole                 -1.444e-01  5.699e-02  -2.534\nYearsSinceLastPromotion             1.297e-01  5.427e-02   2.390\nYearsWithCurrManager               -1.798e-01  5.671e-02  -3.170\n                                   Pr(>|z|)    \n(Intercept)                        0.987410    \nAge                                0.193437    \nBusinessTravelTravel_Frequently    4.02e-05 ***\nBusinessTravelTravel_Rarely        0.010667 *  \nDailyRate                          0.011732 *  \n`DepartmentResearch & Development` 0.984272    \nDepartmentSales                    0.986536    \nDistanceFromHome                   0.000255 ***\nEducation                          0.490339    \n`EducationFieldLife Sciences`      0.111169    \nEducationFieldMarketing            0.197945    \nEducationFieldMedical              0.147641    \nEducationFieldOther                0.157143    \n`EducationFieldTechnical Degree`   0.658385    \nEmployeeCount                            NA    \nEmployeeNumber                     0.194933    \nEnvironmentSatisfactionMedium      6.65e-05 ***\nEnvironmentSatisfactionHigh        6.58e-05 ***\n`EnvironmentSatisfactionVery High` 2.43e-06 ***\nGenderMale                         0.015601 *  \nHourlyRate                         0.355279    \nJobInvolvement                     0.000115 ***\nJobLevel                           0.948156    \n`JobRoleHuman Resources`           0.982959    \n`JobRoleLaboratory Technician`     0.023810 *  \nJobRoleManager                     0.472884    \n`JobRoleManufacturing Director`    0.786830    \n`JobRoleResearch Director`         0.082097 .  \n`JobRoleResearch Scientist`        0.623925    \n`JobRoleSales Executive`           0.025842 *  \n`JobRoleSales Representative`      0.004490 ** \nJobSatisfaction                    0.000265 ***\nMaritalStatusMarried               0.287269    \nMaritalStatusSingle                0.002014 ** \nMonthlyIncome                      0.945273    \nMonthlyRate                        0.996968    \nNumCompaniesWorked                 0.001714 ** \nOverTimeYes                         < 2e-16 ***\nPercentSalaryHike                  0.743877    \nPerformanceRating                  0.947261    \nRelationshipSatisfaction           0.003694 ** \nStockOptionLevel                   0.231003    \nTotalWorkingYears                  0.251447    \nTrainingTimesLastYear              0.025267 *  \nWorkLifeBalanceGood                0.229984    \nWorkLifeBalanceBetter              0.004958 ** \nWorkLifeBalanceBest                0.110300    \nYearsAtCompany                     0.030984 *  \nYearsInCurrentRole                 0.011290 *  \nYearsSinceLastPromotion            0.016843 *  \nYearsWithCurrManager               0.001525 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 919.16  on 1028  degrees of freedom\nResidual deviance: 573.42  on  979  degrees of freedom\nAIC: 673.42\n\nNumber of Fisher Scoring iterations: 15\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  836  82\n       Yes  24  87\n                                          \n               Accuracy : 0.897           \n                 95% CI : (0.8768, 0.9149)\n    No Information Rate : 0.8358          \n    P-Value [Acc > NIR] : 1.313e-08       \n                                          \n                  Kappa : 0.5648          \n                                          \n Mcnemar's Test P-Value : 3.089e-08       \n                                          \n            Sensitivity : 0.51479         \n            Specificity : 0.97209         \n         Pos Pred Value : 0.78378         \n         Neg Pred Value : 0.91068         \n             Prevalence : 0.16424         \n         Detection Rate : 0.08455         \n   Detection Prevalence : 0.10787         \n      Balanced Accuracy : 0.74344         \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\n\n# The CV accuracy of 87.5%.\nattrition_allvars$results$Accuracy\n\n\n\nThis model has a training accuracy of 89.7% and a CV accuracy of 87.9%, an improvement from the previous model. Additionally, the sensitivity of this model is much higher at 51.5%. However, including almost all of the variables makes this model less intuitive. Additionally, it might make the model more prone to overfitting. In order to address this problem, let’s look at what variables are the most crucial to include in this model by making a variable importance plot:\n\n\nvip(attrition_allvars$finalModel, num_features = 30, bar = FALSE)\n\n\n\n\nThe top 5 most important variables in this model are Overtime, EnvironmentSatisfaction, BusinessTravel, JobInvolvement,and DistanceFromHome. Our third model uses logistic regression with these 5 variables.\n3rd model\n\n\nset.seed(253)\n\n\nattrition_bestvars <- train(\n    Attrition ~ OverTime + BusinessTravel + JobInvolvement + DistanceFromHome + EnvironmentSatisfaction  ,\n    data = attrition_train,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 5),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  844 147\n       Yes  16  22\n                                          \n               Accuracy : 0.8416          \n                 95% CI : (0.8178, 0.8634)\n    No Information Rate : 0.8358          \n    P-Value [Acc > NIR] : 0.3244          \n                                          \n                  Kappa : 0.162           \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.13018         \n            Specificity : 0.98140         \n         Pos Pred Value : 0.57895         \n         Neg Pred Value : 0.85166         \n             Prevalence : 0.16424         \n         Detection Rate : 0.02138         \n   Detection Prevalence : 0.03693         \n      Balanced Accuracy : 0.55579         \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\n\n# The CV accuracy is 84.5%.\nattrition_bestvars$results$Accuracy\n\n\n\nThe model ends up having only 84.2% training accuracy, 84.1% CV accuracy, and 13.0% sensitivity, which is similarly poor to our first model. Another method of trying to reduce the number of variables in a model is by using lasso to penalize having lots of variables. For our 4th model, we tried a lasso model to do this and also hopefully get better accuracy and sensitivity rates.\n4th model\n\n\nset.seed(253)\n\nattrition_lasso <- train(\n    Attrition ~ .,\n    data = attrition_train %>% select(-Over18, -StandardHours),\n    method = \"glmnet\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 5),\n    tuneGrid = data.frame(alpha = 1, \n                          lambda = 10^seq(-4, 0, length = 100)),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  838  88\n       Yes  22  81\n                                          \n               Accuracy : 0.8931          \n                 95% CI : (0.8726, 0.9113)\n    No Information Rate : 0.8358          \n    P-Value [Acc > NIR] : 1.080e-07       \n                                          \n                  Kappa : 0.5381          \n                                          \n Mcnemar's Test P-Value : 5.736e-10       \n                                          \n            Sensitivity : 0.47929         \n            Specificity : 0.97442         \n         Pos Pred Value : 0.78641         \n         Neg Pred Value : 0.90497         \n             Prevalence : 0.16424         \n         Detection Rate : 0.07872         \n   Detection Prevalence : 0.10010         \n      Balanced Accuracy : 0.72685         \n                                          \n       'Positive' Class : Yes             \n                                          \n  alpha      lambda  Accuracy     Kappa AccuracySD   KappaSD\n1     1 0.001232847 0.8824485 0.4925641 0.02783599 0.1133576\n\n\n\n# The CV accuracy is 88.2% with a lambda value of 0.001232847.    \nattrition_lasso$bestTune$lambda\n\n\n\n\nModel Evaluation\n\nThis model, using the best lambda (of 0.001232847) had an accuracy rate of 89.31%, which is noticeably higher than the No Information Rate. Most importantly for our model, the sensitivity is 47.9%, meaning, for all the people that truly attrite, we predict correctly 47.9% of the time. Ideally, we would want something higher still, but this sensitivity is much better than the sensitivities in our previous models (besides the all-variable one).\nThe plot below of lambda versus Accuracy shows that Model 4 used the lambda value of 0.001232847, because it resulted in the highest accuracy. However, what happens if we make a new LASSO model with a different lambda value?\n\n\n#plot of lambda values versus accuracy\nattrition_lasso$results %>% \n  ggplot(aes(x = lambda, y = Accuracy)) +\n  geom_line() +\n  scale_x_log10() \n\n\n\n\nNow we try another lasso model with a lambda value of 0.0005336699, to see if this results in a better sensitivity, even if it is at the cost of the accuracy lowering a bit.\n5th Model\n\n\nattrition_lasso_best <- train(\n    Attrition ~ .,\n    data = attrition_train %>% select(-Over18, -StandardHours),\n    method = \"glmnet\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 5),\n    tuneGrid = data.frame(alpha = 1, \n                          lambda = 0.0005336699),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\n\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  838  85\n       Yes  22  84\n                                         \n               Accuracy : 0.896          \n                 95% CI : (0.8757, 0.914)\n    No Information Rate : 0.8358         \n    P-Value [Acc > NIR] : 2.258e-08      \n                                         \n                  Kappa : 0.5545         \n                                         \n Mcnemar's Test P-Value : 2.050e-09      \n                                         \n            Sensitivity : 0.49704        \n            Specificity : 0.97442        \n         Pos Pred Value : 0.79245        \n         Neg Pred Value : 0.90791        \n             Prevalence : 0.16424        \n         Detection Rate : 0.08163        \n   Detection Prevalence : 0.10301        \n      Balanced Accuracy : 0.73573        \n                                         \n       'Positive' Class : Yes            \n                                         \n  alpha       lambda  Accuracy     Kappa AccuracySD   KappaSD\n1     1 0.0005336699 0.8765475 0.4840958 0.02442669 0.1102381\n\nThis model, using the best lambda (of 0.001232847) had a training accuracy rate of 89.6% and CV accuracy rate of 87.3%. The sensitivity also went up to 49.7%. The specificity is at 97.4%, meaning that we predict the people who will remain in their positions almost perfectly. Additionally, 5 coefficients were omitted - DepartmentSales, EmployeeCount, JobLevel, MonthlyIncome, and MonthlyRate. This could potentially help alleviate issues with overfitting on our training data.\nConclusion\nModel\nAccuracy\nSensitivity\nSpecificity\nCV Accuracy\nModel 1: Logistic Regression with Exploratory Variables\n0.8484\n0.14201\n0.98721\n0.8416292\nModel 2: Logistic Regression with All Variables\n0.897\n0.51479\n.97209\n0.8785603\nModel 3: Logistic Regression with Most Important Variables\n0.8416\n0.13018\n0.98140\n0.8406394\nModel 4: LASSO with “Best” Lambda\n0.8931\n0.47929\n0.97442\n0.8824485\nModel 5: LASSO with Different Lambda\n0.896\n0.49704\n0.97442\n0.8726971\nTo pick the final model that we want to use, we need to remember what the goal was of this model-building process: we want to predict who is most likely to attrite. That is, who is most likely to leave their position and not be replaced? The sensitivity will tell us that.The model with the highest sensitivity is Model 2. However, this model may be overfitting to the training data. Model 5 has the second highest sensitivity. We now fit Models 2 and 5 on the testing data to see which is the best:\n\n\n#Model 2\nconfusionMatrix(data = predict(attrition_allvars, newdata = attrition_test, type = \"raw\"), reference = as.factor(attrition_test$Attrition), \n                positive = \"Yes\")\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  357  40\n       Yes  16  28\n                                          \n               Accuracy : 0.873           \n                 95% CI : (0.8383, 0.9026)\n    No Information Rate : 0.8458          \n    P-Value [Acc > NIR] : 0.061980        \n                                          \n                  Kappa : 0.4311          \n                                          \n Mcnemar's Test P-Value : 0.002116        \n                                          \n            Sensitivity : 0.41176         \n            Specificity : 0.95710         \n         Pos Pred Value : 0.63636         \n         Neg Pred Value : 0.89924         \n             Prevalence : 0.15420         \n         Detection Rate : 0.06349         \n   Detection Prevalence : 0.09977         \n      Balanced Accuracy : 0.68443         \n                                          \n       'Positive' Class : Yes             \n                                          \n\n#Model 5\nconfusionMatrix(data = predict(attrition_lasso_best, newdata = attrition_test, type = \"raw\"), reference = as.factor(attrition_test$Attrition), \n                positive = \"Yes\")\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  360  40\n       Yes  13  28\n                                          \n               Accuracy : 0.8798          \n                 95% CI : (0.8458, 0.9087)\n    No Information Rate : 0.8458          \n    P-Value [Acc > NIR] : 0.0251453       \n                                          \n                  Kappa : 0.45            \n                                          \n Mcnemar's Test P-Value : 0.0003551       \n                                          \n            Sensitivity : 0.41176         \n            Specificity : 0.96515         \n         Pos Pred Value : 0.68293         \n         Neg Pred Value : 0.90000         \n             Prevalence : 0.15420         \n         Detection Rate : 0.06349         \n   Detection Prevalence : 0.09297         \n      Balanced Accuracy : 0.68846         \n                                          \n       'Positive' Class : Yes             \n                                          \n\nSurprisingly, their sensitivities are the same! The accuracy on Model 5 is 0.8798 while the accuracy on Model 2 is 0.873, so we can choose Model 5 as our best model for the time being.\n\nOur thoughts\n\nIn the future, we should keep trying to find a model with an even higher sensitivity, because the sensitivity is not “good” in any of these models, and we don’t want to be making many inaccurate predictions about such an important issue. After running the models on the testing data we can conclude that Model 5 (a lasso model fitting all variables with a chosen lambda value) is the “best” model.\n\n\n\n",
    "preview": "posts/2021-03-20-post-2/post-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-20T20:05:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-post-1/",
    "title": "Exploring Metro Transit and Nice Ride Data",
    "description": "Here we looked at transportation data from Metro Transit and Nice Ride bike sharing to see what might impact trends in ridership.  This was my final project for Introduction to Data Science.",
    "author": [
      {
        "name": "Colleen Minnihan, Ellen Graham, Zain Ijaz, Vishal Rana",
        "url": {}
      }
    ],
    "date": "2017-12-06",
    "categories": [],
    "contents": "\n(Preview image source)\nIntroduction\nOur group wanted to know if any relationship exists between Metro Transit Bus Systems and Nice Ride Bikesharing. Through looking at what kinds of information the data gave us, we narrowed down our research to two main questions:\nWhat is the correlation between weather and ridership on Metro Transit and Nice Ride?\nHow do geographical factors relate to Nice Ride and Metro Transit usage?\nFrom there, we took a closer look at each question. While looking at weather, we examined many factors that could influence ridership, such as temperature, precipitation, and snowfall. Through further analysis, we saw that there was no real correlation between precipitation, snowfall, and ridership. Therefore, we focused on temperature, with the sub-question: 1.How does temperature relate to usage? Specifically, we looked at ridership when temperature on a given day in 2016 varied from that area’s average temperature over the last seven years.\nAs for geographical factors, we pruned that broad research question down to: 1. Does distance between bus stops and bike stations correlate with usage? 2. How does Nice Ride usage vary over a day? 3. Does distance relate to usage over a small period of a day? 4. Are usage patterns different in different parts of the city?\nWe were curious if the proximity of a Nice Ride station to a bus stop directly influenced ridership of that Nice Ride station. To gain a deeper understanding, we looked at how Nice Ride usage rose and fell throughout a given day (weekday, Saturday, and Sunday). Lastly, we examined the relationship between proximity of a Nice Ride station to a bus stop and usage of the bike station during five hours of a given day. Initially, we were using three random bus stops for our analysis. We then wondered about two of the most popular Nice Ride stations: one in downtown Minneapolis (likely used for business), and one along Lake Bede Maka Ska (likely used for leisure). Through these more specific research questions, we were able to hypothesize that people combo their transit, meaning they get off a Metro Transit bus and go to a Nice Ride bike station to continue their journey.\nData Collection Process\nInitially, we decided to work with Uber and Lyft data but soon realized that neither of the two were very forthcoming with their data. Our group then started looking for other forms of alternate transit, specifically bike sharing systems within the Twin Cities and eventually found Nice Ride bike sharing data which not only was easily accessible but also was very vast which helped us come up with accurate visualizations and solid conclusions. After obtaining data from Nice Ride, we wanted to know how this form of transit was affected by changes in weather. For that, we looked at various data sources and decided to use weather data from the Minnesota Department of Natural Resources (MN DNR). We chose this specific data because it contained weather information about Minnesota only while other weather data sets contained information pertaining to states other than Minnesota as well and making effective visualizations with the other data sources was not very feasible. Our research question was about metro transit vs. alternate forms of transit and hence, we used Metro Transit data provided to us by Mr. Eric Lind combined with the Nice Ride and MN DNR data to tackle our research topic.\nDatasets we will use\nmetroStops\nData source: Metro Transit Data description: Location of each bus stop (with site ID, city, latitude, longitude, etc.) Data limitations: bus stops that exist from 2014-2017 Data dimensions: 14,919 x 12\nmetroRidership\nData source: Metro Transit\nData description: Gives the day, if it’s a holiday, route number, route type, number of trips, and total number of riders on that day\nData limitations: from January 2014 - October 2017\nData dimensions: 131,078 x 10\nNicerideRidership\nData Source: Nice Ride 2016 data\nData Description: Data contains information about trips: the start and end stations of each trip, the total time it took to complete the journey, and whether the passenger was a casual rider or a member.\nData Limitations: Does not contain information about metro rides in December, January, February, and March. Only has 432283 rows with data in them, the rest are empty and this data has to be cleaned accordingly before use.\nData dimensions: >432,284x8\nNiceRideStops\nData Source: Nice Ride 2016 data\nData Description: Data contains names of Metro Bus Stations, and the exact coordinates of each station (latitude and longitude)\nData Limitations: This dataset contains everything we require to come up with comprehensive visualizations hence, there are no limitations to this data.\nData Dimensions: 202 x 6\nMinneapolisWeather\nData Source: “http://www.dnr.state.mn.us/climate/twin_cities/listings.html”, the Department of Natural Resources has data going back to 1871 on the weather in Minneapolis/St. Paul. All data after 1938 is from MSP airport\nData Description: Gives the date, max temperature, min temperature, amount of percpititaion, amount of snow, and amount of snow on the ground\nData Limitations The specific data we’re using is the weather data between 2010 and 2017. It’s missing the average tempearture for a day, but the min and max are probably enough to work with\nData dimensions: 2884 x 6\n\n\n\n\n\n\nRead in data on MetroTransit\n\n\n\nWeather analysis\n\n\n\n\n\n\n\n\n\nVisualization of how ridership is dependent on temperature of a day and how it depends on divergence from historical temperature on that day.\n\n\n\nWe see that ridership does correlate wtih absolute temperature peaking at around 50 degrees F.\n\n\n\nRidership drops off significantly on days where weather is significantly colder than norms, and is approximately constant for temps above norms.\nDoing same weather analysis with ridership\n\n\n\nLook at how NiceRide ridership correlates to temperature.\n\n\n\nUnsuprisingly, more people ride bikes in warmer weather.\nNow look at how ridership correlates to departure from climate normals.\n\n\n\nPeople ride much less on days with much lower temperatures than normals, and more on days higher than normals.\nGeography and Bike and Bus Ridership\n\n\n\nFirst, let’s visualize where all of the Minneapolis Metro Transit bus stops and Nice Ride bike stations are located.\n\n\n\nHere, we can see the many Metro Transit stations that are in Minneapolis, with a large cluster of them in located in downtown Minneapolis.\n\n\n\nWe can see that there are much fewer Nice Ride stations than Metro Transit bus stops, with most stations clustered in the center of Minneapolis and near parks.\n\n\n\nTo visualize Niceride stations’ distances to bus stops and their ridership.\n\n\n\nNow that we have a basic understanding of where the Minneapolis Metro Transit stop and Nice Ride Stations are located, we wanted to see how distance from a bike station to a bus stop influenced ridership. Each dot on the map represents one Nice Ride station. The sizing of the dots is based on number of total riders over the course of 2016 at that station. The bigger the dot, the more that station was used. The color of the dots represents how close that station was to a bus stop. If the dot color is at the blue end of the spectrum, the Nice Ride station is within 200 meters from a bus stop. If the dot color is orange, it is around 600 meters away from a bus stop. Through this visualization, we can see that the closer a bike station is to a bus stop, the more usage it gets.\n\n\n\n\n\n\n\n\n\nNext, select a set of 3 random bus stations and understand how stations less than 500 meters from these stations change in usage over time.\n\n\n\nVisualize how bike stations near to selected bus stations vary in usage between 3PM and 8PM.\n\n\n\n\n\n#calculates amount of trips the graph represents\nniceRideSampledClose%>%\n   count(site_id)\n\n\n  site_id    n\n1  Stop 1  603\n2  Stop 2 1359\n3  Stop 3  266\n\nWe see spikes and dips in usage that at regular intervals, perhaps indicating a relationship between a bus letting out people and those people using NiceRide.\nFind representative bike station for downtown St. Paul, Minneapolis, and Lake Bde Maka Ska.\n\n\n\nNext, find closest bus stop to these stations.\n\n\n\n\n\n#Gives total number of trips for these two stations in this time period\nniceRideRep%>%\n   count(stationName)\n\n\n        stationName    n\n1 Lake Bde Maka Ska 2888\n2       Minneapolis 3608\n\n\n\n\n",
    "preview": "posts/2021-03-19-post-1/aline-bus.jpg",
    "last_modified": "2021-03-24T21:08:42-05:00",
    "input_file": {}
  }
]
